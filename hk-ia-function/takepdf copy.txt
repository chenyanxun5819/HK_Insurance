from google.cloud import storage
import os, tempfile, sqlite3, requests, pdfplumber, random, string
from datetime import datetime

IA_BASE_URL = "https://www.ia.org.hk/en/legislative_framework/circulars/antimoney_laundering/files/"

# ---------- DB åŸºç¤Ž ----------

def _connect(db_path):
    return sqlite3.connect(db_path)

def ensure_db_exists(local_path):
    """è‹¥ DB ä¸å­˜åœ¨ï¼Œå»ºç«‹ç©º DB èˆ‡åŸºæœ¬è¡¨çµæ§‹ã€‚è‹¥å­˜åœ¨ï¼Œç¢ºä¿ schema å®Œæ•´ã€‚"""
    first_time = not os.path.exists(local_path)
    conn = _connect(local_path)
    conn.execute("""
        CREATE TABLE IF NOT EXISTS profiles (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            year INTEGER,
            name TEXT,
            nationality TEXT,
            passport_no TEXT,
            source_pdf TEXT,
            created_at TEXT
        )
    """)
    # schema å‡ç´šï¼šç¢ºä¿æ‰€æœ‰æ¬„ä½å­˜åœ¨ï¼ˆé˜²æ­¢èˆŠ DB ç¼ºæ¬„ä½ï¼‰
    _ensure_column(conn, "profiles", "nationality", "TEXT")
    _ensure_column(conn, "profiles", "passport_no", "TEXT")
    _ensure_column(conn, "profiles", "created_at", "TEXT")
    conn.execute("""
        CREATE TABLE IF NOT EXISTS processed_files (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            source_pdf TEXT UNIQUE
        )
    """)
    conn.commit()
    conn.close()
    if first_time:
        print("âœ… å»ºç«‹æ–° DB èˆ‡è¡¨çµæ§‹å®Œæˆ")

def _ensure_column(conn, table, column, type_sql):
    cur = conn.execute(f"PRAGMA table_info({table})")
    cols = [row[1] for row in cur.fetchall()]
    if column not in cols:
        conn.execute(f"ALTER TABLE {table} ADD COLUMN {column} {type_sql}")

def download_db(bucket_name, db_file):
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(db_file)
    local_path = os.path.join(tempfile.gettempdir(), db_file)
    try:
        blob.download_to_filename(local_path)
        print(f"âœ… å·²ä¸‹è¼‰ DB: {local_path}")
    except Exception as e:
        print(f"âš ï¸ ä¸‹è¼‰ DB å¤±æ•—ï¼ˆå¯èƒ½ä¸å­˜åœ¨ï¼‰: {e}")
        ensure_db_exists(local_path)
    else:
        # å³ä½¿æœ‰æª”ï¼Œä¹Ÿç¢ºä¿ schema å®Œæ•´
        ensure_db_exists(local_path)
    return local_path

def upload_db(bucket_name, db_file, local_path):
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(db_file)
    blob.upload_from_filename(local_path)
    print(f"âœ… å·²ä¸Šå‚³ DB åˆ° {bucket_name}/{db_file}")

# ---------- æ›´æ–°æµç¨‹ï¼ˆéª¨æž¶ï¼šç©º DB å…¨é‡ï¼Œéžç©ºæŠ“æœ€æ–°å¹´åº¦ï¼‰----------

def get_existing_years(db_path):
    conn = _connect(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT DISTINCT year FROM profiles WHERE year IS NOT NULL ORDER BY year")
    years = [row[0] for row in cursor.fetchall()]
    conn.close()
    return years

def fetch_pdfs_for_year(year):
    # TODO: é€™è£¡æ›æˆçœŸå¯¦çˆ¬èŸ²é‚è¼¯
    # ç¯„ä¾‹å›žå‚³ä¸€ç­†æ¸¬è©¦ PDF URL
    return [f"{IA_BASE_URL}AML_{year}_example.pdf"]

def process_pdfs(pdf_urls, db_path, year):
    conn = _connect(db_path)
    for url in pdf_urls:
        try:
            r = requests.get(url, timeout=30)
        except Exception as e:
            print(f"âŒ é€£ç·šéŒ¯èª¤: {url} -> {e}")
            continue
        if r.status_code == 200:
            pdf_path = os.path.join(tempfile.gettempdir(), os.path.basename(url))
            with open(pdf_path, "wb") as f:
                f.write(r.content)
            try:
                with pdfplumber.open(pdf_path) as pdf:
                    text = "\n".join(page.extract_text() or "" for page in pdf.pages)
                # å¯«å…¥ä¸€ç­†ç°¡åŒ–è³‡æ–™ï¼ˆçœŸå¯¦é‚è¼¯è«‹æ›¿æ›ï¼‰
                conn.execute("""
                    INSERT INTO profiles (year, name, nationality, passport_no, source_pdf, created_at)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (year, f"Sample Name {year}", "china", _rand_passport(), url, _now()))
                conn.commit()
                print(f"ðŸ“„ è§£æžå®Œæˆä¸¦å¯«å…¥ DB: {url}")
            except Exception as e:
                print(f"âŒ PDF è§£æž/å¯«å…¥éŒ¯èª¤: {e}")
        else:
            print(f"âŒ ç„¡æ³•ä¸‹è¼‰ {url}, status={r.status_code}")
    conn.close()

def run_crawler(bucket_name, db_file):
    db_path = download_db(bucket_name, db_file)
    years = get_existing_years(db_path)
    if not years:
        years_to_fetch = list(range(2015, 2026))  # TODO: ä¾å¯¦éš›æœ€èˆŠå¹´åº¦èª¿æ•´
        print(f"ðŸ“… ç©º DB â†’ å…¨é‡æŠ“å–: {years_to_fetch}")
    else:
        years_to_fetch = [max(years) + 1]
        print(f"ðŸ“… DB å·²æœ‰ {years} â†’ æŠ“å–æœ€æ–°å¹´åº¦: {years_to_fetch}")
    for y in years_to_fetch:
        pdf_urls = fetch_pdfs_for_year(y)
        process_pdfs(pdf_urls, db_path, y)
    upload_db(bucket_name, db_file, db_path)

# ---------- æŸ¥è©¢èˆ‡æ¸¬è©¦è³‡æ–™ ----------

def query_name(bucket_name, db_file, name):
    db_path = download_db(bucket_name, db_file)
    conn = _connect(db_path)
    cursor = conn.cursor()
    cursor.execute("""
        SELECT name, nationality, passport_no
        FROM profiles
        WHERE name LIKE ? COLLATE NOCASE
    """, (f"%{name}%",))
    rows = cursor.fetchall()
    conn.close()
    # æ ¼å¼åŒ–æˆ "å§“å | åœ‹ç± | è­·ç…§è™Ÿç¢¼"
    formatted = [f"{r[0]} | {r[1]} | {r[2]}" for r in rows]
    return (len(formatted) > 0, formatted)


