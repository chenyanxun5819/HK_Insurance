from google.cloud import storage
import os, tempfile, sqlite3, requests, pdfplumber, random, string
from datetime import datetime
from bs4 import BeautifulSoup
from urllib.parse import urljoin

IA_INDEX_URL = "https://www.ia.org.hk/en/legislative_framework/circulars/antimoney_laundering/circulars_on_anti-money_laundering_matters.html"
IA_BASE_URL = "https://www.ia.org.hk/en/legislative_framework/circulars/antimoney_laundering/"

# ---------- DB åŸºç¤ ----------
def _connect(db_path):
    return sqlite3.connect(db_path)

def ensure_db_exists(local_path):
    first_time = not os.path.exists(local_path)
    conn = _connect(local_path)
    conn.execute("""
        CREATE TABLE IF NOT EXISTS profiles (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            year INTEGER,
            name TEXT,
            nationality TEXT,
            passport_no TEXT,
            source_pdf TEXT,
            source_url TEXT,
            created_at TEXT
        )
    """)
    _ensure_column(conn, "profiles", "nationality", "TEXT")
    _ensure_column(conn, "profiles", "passport_no", "TEXT")
    _ensure_column(conn, "profiles", "source_url", "TEXT")
    _ensure_column(conn, "profiles", "created_at", "TEXT")
    conn.execute("""
        CREATE TABLE IF NOT EXISTS processed_files (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            file_path TEXT,
            source_pdf TEXT,
            source_url TEXT,
            processed_at TEXT
        )
    """)
    conn.commit()
    conn.close()
    if first_time:
        print("âœ… å»ºç«‹æ–° DB èˆ‡è¡¨çµæ§‹å®Œæˆ")

def _ensure_column(conn, table, column, type_sql):
    cur = conn.execute(f"PRAGMA table_info({table})")
    cols = [row[1] for row in cur.fetchall()]
    if column not in cols:
        conn.execute(f"ALTER TABLE {table} ADD COLUMN {column} {type_sql}")

def download_db(bucket_name, db_file):
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(db_file)
    local_path = os.path.join(tempfile.gettempdir(), db_file)
    try:
        blob.download_to_filename(local_path)
        print(f"âœ… å·²ä¸‹è¼‰ DB: {local_path}")
    except Exception as e:
        print(f"âš ï¸ ä¸‹è¼‰ DB å¤±æ•—ï¼ˆå¯èƒ½ä¸å­˜åœ¨ï¼‰: {e}")
        ensure_db_exists(local_path)
    else:
        ensure_db_exists(local_path)
    return local_path

def upload_db(bucket_name, db_file, local_path):
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(db_file)
    blob.upload_from_filename(local_path)
    print(f"âœ… å·²ä¸Šå‚³ DB åˆ° {bucket_name}/{db_file}")

# ---------- çœŸå¯¦çˆ¬èŸ² ----------
def fetch_pdfs_for_year(year):
    """å¾å¹´ä»½å°è¦½é æ‰¾åˆ°è©²å¹´åº¦çš„ PDF é€£çµ"""
    print(f"ğŸ” æŠ“å– {year} å¹´ PDF é€£çµ...")
    # å…ˆæŠ“å¹´ä»½å°è¦½é 
    index_resp = requests.get(IA_INDEX_URL, timeout=30)
    index_resp.raise_for_status()
    index_soup = BeautifulSoup(index_resp.text, "html.parser")

    # æ‰¾åˆ°è©²å¹´åº¦çš„å­é é¢é€£çµ
    year_link = None
    for a in index_soup.find_all("a", href=True):
        href = a["href"]
        if f"circulars_on_anti-money_laundering_matters_{year}" in href and href.endswith(".html"):
            year_link = urljoin(IA_BASE_URL, href)
            break

    if not year_link:
        # å‚™ç”¨æ–¹æ¡ˆï¼šç›´æ¥ç”¨å¹´ä»½ URL æ ¼å¼
        year_link = f"{IA_BASE_URL}circulars_on_anti-money_laundering_matters_{year}.html"
        print(f"ğŸ”„ ä¸»é é¢æœªæ‰¾åˆ°å¹´ä»½é€£çµï¼Œä½¿ç”¨å‚™ç”¨ URL: {year_link}")

    # é€²å…¥å¹´åº¦é é¢æ‰¾ PDF
    year_resp = requests.get(year_link, timeout=30)
    year_resp.raise_for_status()
    year_soup = BeautifulSoup(year_resp.text, "html.parser")

    pdf_links = []
    for a in year_soup.find_all("a", href=True):
        href = a["href"]
        if href.lower().endswith(".pdf"):
            full_pdf_url = urljoin(year_link, href)
            pdf_links.append(full_pdf_url)

    print(f"ğŸ“„ {year} å¹´æ‰¾åˆ° {len(pdf_links)} å€‹ PDF")
    return pdf_links

def process_pdfs(pdf_urls, db_path, year):
    conn = _connect(db_path)
    processed_count = 0
    
    print(f"ğŸ“Š é–‹å§‹è™•ç† {len(pdf_urls)} å€‹ PDF æª”æ¡ˆ")
    
    for i, url in enumerate(pdf_urls):
        print(f"ï¿½ è™•ç† PDF {i+1}/{len(pdf_urls)}: {os.path.basename(url)}")
        
        # æ¯å€‹ PDF å–®ç¨è™•ç†ï¼Œç«‹å³å­˜å…¥ DB ä¸¦é‡‹æ”¾è¨˜æ†¶é«”
        if process_single_pdf(url, conn, year):
            processed_count += 1
            # ç«‹å³æäº¤åˆ°è³‡æ–™åº«
            conn.commit()
            print(f"âœ… PDF {i+1} è™•ç†å®Œæˆï¼Œå·²å­˜å…¥ DB")
        else:
            print(f"â© PDF {i+1} è·³éæˆ–å¤±æ•—")
        
        # æ¯ 5 å€‹ PDF å¾Œå¼·åˆ¶åƒåœ¾å›æ”¶
        if (i + 1) % 5 == 0:
            import gc
            gc.collect()
            print(f"ğŸ§¹ å·²æ¸…ç†è¨˜æ†¶é«” (è™•ç†äº† {i+1} å€‹ PDF)")
    
    conn.close()
    print(f"ğŸ‰ å…¨éƒ¨å®Œæˆï¼å…±æˆåŠŸè™•ç† {processed_count} å€‹ PDF")
    return processed_count

def process_single_pdf(url, conn, year):
    """è™•ç†å–®å€‹ PDF æª”æ¡ˆï¼Œç«‹å³å­˜å…¥ DB ä¸¦é‡‹æ”¾è¨˜æ†¶é«”"""
    # æª¢æŸ¥æ˜¯å¦å·²è™•ç†é - å…¼å®¹èˆŠè³‡æ–™åº«çµæ§‹
    try:
        cur = conn.execute("SELECT 1 FROM processed_files WHERE source_pdf=?", (url,))
    except sqlite3.OperationalError:
        # å¦‚æœèˆŠæ¬„ä½ä¸å­˜åœ¨ï¼Œå˜—è©¦æ–°æ¬„ä½çµæ§‹
        try:
            cur = conn.execute("SELECT 1 FROM processed_files WHERE source_url=?", (url,))
        except sqlite3.OperationalError:
            # å¦‚æœè¡¨æ ¼ä¸å­˜åœ¨ï¼Œå»ºç«‹æ–°è¡¨æ ¼
            conn.execute("""
                CREATE TABLE IF NOT EXISTS processed_files (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    file_path TEXT,
                    source_pdf TEXT,
                    source_url TEXT,
                    processed_at TEXT
                )
            """)
            conn.commit()
            cur = conn.execute("SELECT 1 FROM processed_files WHERE source_url=?", (url,))
    
    if cur.fetchone():
        print(f"â© å·²è™•ç†éï¼Œè·³é: {os.path.basename(url)}")
        return False

    pdf_path = None
    try:
        # ä½¿ç”¨ä¸²æµä¸‹è¼‰ï¼Œæ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨
        print(f"â¬‡ï¸ ä¸‹è¼‰ PDF: {os.path.basename(url)}")
        r = requests.get(url, timeout=60, stream=True)
        r.raise_for_status()
        
        pdf_path = os.path.join(tempfile.gettempdir(), f"temp_{os.getpid()}_{os.path.basename(url)}")
        
        # ä¸²æµå¯«å…¥æª”æ¡ˆ
        with open(pdf_path, "wb") as f:
            for chunk in r.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
        
        # ç«‹å³é‡‹æ”¾ä¸‹è¼‰ç‰©ä»¶
        r.close()
        del r
        
    except Exception as e:
        print(f"âŒ ä¸‹è¼‰å¤±æ•— {os.path.basename(url)}: {e}")
        return False

    entries_count = 0
    try:
        print(f"ğŸ” è§£æ PDF: {os.path.basename(url)}")
        # è™•ç† PDF
        with pdfplumber.open(pdf_path) as pdf:
            # é™åˆ¶è™•ç†çš„é æ•¸ï¼Œé¿å…è¨˜æ†¶é«”çˆ†ç‚¸  
            max_pages = min(30, len(pdf.pages))  # æœ€å¤š 30 é 
            print(f"ğŸ“– PDF å…± {len(pdf.pages)} é ï¼Œè™•ç†å‰ {max_pages} é ")
            
            for page_num in range(max_pages):
                page = pdf.pages[page_num]
                text = page.extract_text() or ""
                
                # ç«‹å³è™•ç†ç•¶å‰é é¢çš„æ¢ç›®
                entries = extract_sanction_entries(text)
                for entry in entries:
                    if entry['name'] and entry['name'] != 'Unknown' and len(entry['name']) > 3:
                        try:
                            # å˜—è©¦æ–°è³‡æ–™åº«çµæ§‹
                            conn.execute("""
                                INSERT INTO profiles (year, name, nationality, passport_no, source_pdf, source_url, created_at) 
                                VALUES (?, ?, ?, ?, ?, ?, ?)
                            """, (year, entry['name'], entry['nationality'], entry['passport_no'], url, url, _now()))
                        except sqlite3.OperationalError:
                            # å¦‚æœå¤±æ•—ï¼Œä½¿ç”¨èˆŠè³‡æ–™åº«çµæ§‹
                            conn.execute("""
                                INSERT INTO profiles (year, name, nationality, passport_no, source_pdf, created_at) 
                                VALUES (?, ?, ?, ?, ?, ?)
                            """, (year, entry['name'], entry['nationality'], entry['passport_no'], url, _now()))
                        entries_count += 1
                
                # ç«‹å³é‡‹æ”¾ç•¶å‰é é¢çš„è¨˜æ†¶é«”
                del text, entries
                
                # æ¯ 10 é æäº¤ä¸€æ¬¡ï¼Œé¿å…è³‡æ–™ä¸Ÿå¤±
                if (page_num + 1) % 10 == 0:
                    conn.commit()
                    import gc
                    gc.collect()
                    print(f"  ğŸ’¾ å·²è™•ç† {page_num + 1} é ï¼Œæäº¤åˆ° DB")
        
        # è¨˜éŒ„å·²è™•ç†çš„æª”æ¡ˆ - å…¼å®¹æ–°èˆŠè³‡æ–™åº«çµæ§‹
        try:
            conn.execute("INSERT INTO processed_files (source_pdf, source_url, processed_at) VALUES (?, ?, ?)", (url, url, _now()))
        except sqlite3.OperationalError:
            # å¦‚æœæ–°æ¬„ä½ä¸å­˜åœ¨ï¼Œä½¿ç”¨èˆŠæ ¼å¼
            conn.execute("INSERT INTO processed_files (source_pdf) VALUES (?)", (url,))
        print(f"ğŸ“ å¾ {os.path.basename(url)} æå–äº† {entries_count} å€‹æ¢ç›®")
        return True
        
    except Exception as e:
        print(f"âŒ PDF è§£æéŒ¯èª¤ {os.path.basename(url)}: {e}")
        return False
    finally:
        # ç¢ºä¿æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
        if pdf_path and os.path.exists(pdf_path):
            try:
                os.remove(pdf_path)
                print(f"ğŸ—‘ï¸ å·²æ¸…ç†è‡¨æ™‚æª”æ¡ˆ: {os.path.basename(pdf_path)}")
            except:
                pass
        
        # å¼·åˆ¶åƒåœ¾å›æ”¶
        import gc
        gc.collect()

# ---------- æ›´æ–°æµç¨‹ ----------
def get_existing_years(db_path):
    conn = _connect(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT DISTINCT year FROM profiles WHERE year IS NOT NULL ORDER BY year")
    years = [row[0] for row in cursor.fetchall()]
    conn.close()
    return years

def run_crawler(bucket_name, db_file):
    start_time = datetime.now()
    
    # 1. å¾é›²ç«¯ä¸‹è¼‰ç¾æœ‰çš„ DB æª”æ¡ˆåˆ°æœ¬åœ°è‡¨æ™‚ç›®éŒ„
    db_path = download_db(bucket_name, db_file)
    
    # 2. æª¢æŸ¥å·²æœ‰çš„å¹´ä»½è³‡æ–™
    years = get_existing_years(db_path)
    current_year = datetime.now().year
    
    # ç°¡åŒ–é‚è¼¯ï¼šåªè™•ç†ç•¶å‰å¹´ä»½ï¼ˆè‡ªå‹•é©æ‡‰æœªä¾†å¹´ä»½ï¼‰
    # çŠ¯ç½ªåå–®ä¸æº¯åŠéå¾€ï¼Œéå»æ²’æœ‰çš„å¹´ä»½å°±æ°¸é æ²’æœ‰
    # 2025å¹´å¯ä½œç‚ºæ¸¬è©¦å¹´ä»½ï¼Œä¹‹å¾Œç³»çµ±æœƒè‡ªå‹•è™•ç†2026ã€2027ç­‰
    
    if not years:
        # ç©º DBï¼Œå¾2001å¹´é–‹å§‹è™•ç†åˆ°ç•¶å‰å¹´ä»½
        years_to_fetch = list(range(2001, current_year + 1))
        print(f"ğŸ“… é¦–æ¬¡åŸ·è¡Œ â†’ è™•ç†æ‰€æœ‰å¹´ä»½: {len(years_to_fetch)} å¹´ ({min(years_to_fetch)}-{max(years_to_fetch)})")
    else:
        # æœ‰è³‡æ–™ï¼Œåªæª¢æŸ¥ç•¶å‰å¹´ä»½æ˜¯å¦æœ‰æ–°è³‡æ–™
        if current_year not in years:
            years_to_fetch = [current_year]
            print(f"ğŸ“… DB å·²æœ‰å¹´ä»½: {sorted(years)}")
            print(f"ğŸ“… æª¢æŸ¥æ–°å¹´ä»½: {current_year}")
        else:
            years_to_fetch = [current_year]  # å³ä½¿æœ‰è³‡æ–™ï¼Œä¹Ÿæª¢æŸ¥ç•¶å‰å¹´ä»½çš„æ–°æª”æ¡ˆ
            print(f"ğŸ“… DB å·²æœ‰å¹´ä»½: {sorted(years)}")
            print(f"ğŸ“… æª¢æŸ¥ç•¶å‰å¹´ä»½æ–°æª”æ¡ˆ: {current_year}")

    if not years_to_fetch:
        print("ğŸ“… æ²’æœ‰æ–°è³‡æ–™éœ€è¦è™•ç†")
        return

    print(f"ğŸ¯ è¨ˆåŠƒè™•ç† {len(years_to_fetch)} å€‹å¹´ä»½")
    
    total_files = 0
    for i, y in enumerate(years_to_fetch):
        print(f"ğŸ”„ é–‹å§‹è™•ç† {y} å¹´ ({i+1}/{len(years_to_fetch)})...")
        
        # 3. æŠ“å–è©²å¹´ä»½çš„æ‰€æœ‰ PDF é€£çµ
        pdf_urls = fetch_pdfs_for_year(y)
        if pdf_urls:
            print(f"ğŸ“„ {y} å¹´æ‰¾åˆ° {len(pdf_urls)} å€‹ PDF æª”æ¡ˆ")
            
            # 4. é€å€‹è™•ç† PDFï¼ˆè‡ªå‹•è·³éå·²è™•ç†çš„ï¼‰
            processed = process_pdfs(pdf_urls, db_path, y)
            total_files += processed
            print(f"ğŸ“Š {y} å¹´è™•ç†å®Œæˆï¼Œæ–°å¢ {processed} å€‹æª”æ¡ˆ")
            
            # æ¯å¹´è™•ç†å®Œå¾Œä¸Šå‚³ä¸€æ¬¡ï¼Œé¿å…è³‡æ–™ä¸Ÿå¤±
            upload_db(bucket_name, db_file, db_path)
            print(f"â˜ï¸ {y} å¹´è³‡æ–™å·²å‚™ä»½åˆ°é›²ç«¯")
        else:
            print(f"âš ï¸ {y} å¹´æ²’æœ‰æ‰¾åˆ° PDF æª”æ¡ˆ")
        
        # æª¢æŸ¥åŸ·è¡Œæ™‚é–“ï¼Œé¿å…è¶…æ™‚
        elapsed = (datetime.now() - start_time).total_seconds()
        if elapsed > 3000:  # 50åˆ†é˜å¾Œåœæ­¢ï¼Œé¿å…Cloud Runè¶…æ™‚
            print(f"â° åŸ·è¡Œæ™‚é–“éé•· ({elapsed:.0f}s)ï¼Œæš«åœè™•ç†ã€‚å‰©é¤˜å¹´ä»½ä¸‹æ¬¡åŸ·è¡Œæ™‚ç¹¼çºŒã€‚")
            break

    # 5. æœ€çµ‚ä¸Šå‚³
    upload_db(bucket_name, db_file, db_path)
    
    elapsed = (datetime.now() - start_time).total_seconds()
    print(f"âœ… æ›´æ–°å®Œæˆï¼å…±è™•ç† {total_files} å€‹æ–°æª”æ¡ˆï¼Œè€—æ™‚ {elapsed:.2f} ç§’")

# ---------- æŸ¥è©¢ ----------
def query_name(bucket_name, db_file, name):
    db_path = download_db(bucket_name, db_file)
    conn = _connect(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT name, nationality, passport_no FROM profiles WHERE name LIKE ? COLLATE NOCASE", (f"%{name}%",))
    rows = cursor.fetchall()
    conn.close()
    formatted = [f"{r[0]} | {r[1]} | {r[2]}" for r in rows]
    return (len(formatted) > 0, formatted)

def get_profiles_paginated(bucket_name, db_file, page=1, per_page=20, nationality=None, search_name=None):
    """åˆ†é ç²å–åˆ¶è£åå–®"""
    db_path = download_db(bucket_name, db_file)
    
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # æª¢æŸ¥è¡¨å - æ–°ç‰ˆæœ¬ç”¨aml_profilesï¼ŒèˆŠç‰ˆæœ¬ç”¨profiles
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
        tables = [table[0] for table in cursor.fetchall()]
        table_name = 'aml_profiles' if 'aml_profiles' in tables else 'profiles'
        
        # æ§‹å»ºæŸ¥è©¢æ¢ä»¶
        where_conditions = []
        params = []
        
        if nationality:
            where_conditions.append("nationality = ?")
            params.append(nationality)
            
        if search_name:
            where_conditions.append("name LIKE ?")
            params.append(f"%{search_name}%")
        
        where_clause = " WHERE " + " AND ".join(where_conditions) if where_conditions else ""
        
        # æŸ¥è©¢ç¸½æ•¸
        count_query = f"SELECT COUNT(*) FROM {table_name}{where_clause}"
        cursor.execute(count_query, params)
        total = cursor.fetchone()[0]
        
        # è¨ˆç®—åˆ†é 
        total_pages = (total + per_page - 1) // per_page
        offset = (page - 1) * per_page
        
        # æŸ¥è©¢æ•¸æ“š - å…ˆæª¢æŸ¥æ¬„ä½æ˜¯å¦å­˜åœ¨
        cursor.execute(f"PRAGMA table_info({table_name})")
        columns = [col[1] for col in cursor.fetchall()]
        has_source_fields = 'source_pdf' in columns and 'source_url' in columns
        
        if has_source_fields:
            query = f"""
            SELECT name, nationality, passport_no, year, source_pdf, source_url
            FROM {table_name}{where_clause}
            ORDER BY year DESC, name
            LIMIT ? OFFSET ?
            """
        else:
            query = f"""
            SELECT name, nationality, passport_no, year
            FROM {table_name}{where_clause}
            ORDER BY year DESC, name
            LIMIT ? OFFSET ?
            """
            
        cursor.execute(query, params + [per_page, offset])
        rows = cursor.fetchall()
        
        profiles = []
        for row in rows:
            if has_source_fields:
                name, nationality, passport_no, year, source_pdf, source_url = row
                profiles.append({
                    'name': name,
                    'nationality': nationality,
                    'passport_no': passport_no,
                    'year': year,
                    'source_pdf': source_pdf or '',
                    'source_url': source_url or ''
                })
            else:
                name, nationality, passport_no, year = row
                profiles.append({
                    'name': name,
                    'nationality': nationality,
                    'passport_no': passport_no,
                    'year': year,
                    'source_pdf': '',
                    'source_url': ''
                })
        
        conn.close()
        
        return {
            'profiles': profiles,
            'total': total,
            'page': page,
            'per_page': per_page,
            'total_pages': total_pages,
            'has_prev': page > 1,
            'has_next': page < total_pages
        }
        
    except Exception as e:
        print(f"åˆ†é æŸ¥è©¢éŒ¯èª¤: {e}")
        return {
            'profiles': [],
            'total': 0,
            'page': 1,
            'per_page': per_page,
            'total_pages': 0,
            'has_prev': False,
            'has_next': False
        }

def get_stats(bucket_name, db_file):
    """ç²å–çµ±è¨ˆä¿¡æ¯"""
    db_path = download_db(bucket_name, db_file)
    
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # æª¢æŸ¥è¡¨å - æ–°ç‰ˆæœ¬ç”¨aml_profilesï¼ŒèˆŠç‰ˆæœ¬ç”¨profiles
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
        tables = [table[0] for table in cursor.fetchall()]
        table_name = 'aml_profiles' if 'aml_profiles' in tables else 'profiles'
        
        # ç¸½æ•¸çµ±è¨ˆ
        cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
        total_profiles = cursor.fetchone()[0]
        
        # å¹´ä»½çµ±è¨ˆ
        cursor.execute(f"SELECT year, COUNT(*) as count FROM {table_name} GROUP BY year ORDER BY year DESC")
        year_stats = cursor.fetchall()
        
        # åœ‹ç±çµ±è¨ˆ
        cursor.execute(f"SELECT nationality, COUNT(*) as count FROM {table_name} GROUP BY nationality ORDER BY count DESC LIMIT 10")
        nationality_stats = cursor.fetchall()
        
        # æœ€æ–°æ•¸æ“šå¹´ä»½
        cursor.execute(f"SELECT MAX(year) FROM {table_name}")
        latest_year = cursor.fetchone()[0]
        
        conn.close()
        
        return {
            'total_profiles': total_profiles,
            'latest_year': latest_year,
            'year_stats': year_stats,
            'nationality_stats': nationality_stats
        }
        
    except Exception as e:
        print(f"çµ±è¨ˆæŸ¥è©¢éŒ¯èª¤: {e}")
        return {
            'total_profiles': 0,
            'latest_year': None,
            'year_stats': [],
            'nationality_stats': []
        }

# ---------- å·¥å…· ----------
def _rand_passport():
    return "E" + "".join(random.choice(string.digits) for _ in range(8))

def _now():
    return datetime.utcnow().isoformat(timespec="seconds") + "Z"

def extract_sanction_entries(text):
    """å¾ PDF æ–‡å­—ä¸­æå–åˆ¶è£åå–®æ¢ç›®"""
    import re
    
    entries = []
    lines = text.split('\n')
    current_entry = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        # æª¢æŸ¥æ˜¯å¦æ˜¯æ–°æ¢ç›®é–‹å§‹ (QDi.XXX æ ¼å¼)
        if re.match(r'^QDi\.\d+', line):
            # ä¿å­˜å‰ä¸€å€‹æ¢ç›®
            if current_entry:
                entries.append(finalize_entry(current_entry))
            
            # é–‹å§‹æ–°æ¢ç›®
            current_entry = {
                'name': '',
                'nationality': 'Unknown',
                'passport_no': 'na',
                'raw_text': line
            }
            
            # æå–å§“å - æ›´éˆæ´»çš„æ¨¡å¼
            name_patterns = [
                r'Name:\s*1:\s*([A-Z\s]+?)\s*2:\s*([A-Z\s]*?)\s*3:\s*([A-Z\s]*?)\s*4:\s*([A-Z\s]*?)(?:\s*Title:|DOB:|Designation:|$)',
                r'Name:\s*1:\s*(\w+)\s*2:\s*(\w*)\s*3:\s*(\w*)\s*4:\s*(\w*)',
                r'Name:\s*([A-Z][A-Z\s]+?)(?:Title:|DOB:|Designation:|$)'
            ]
            
            for pattern in name_patterns:
                name_match = re.search(pattern, line)
                if name_match:
                    if len(name_match.groups()) > 1:
                        # å¤šå€‹çµ„çš„æƒ…æ³
                        name_parts = [part.strip() for part in name_match.groups() if part and part.strip()]
                        current_entry['name'] = ' '.join(name_parts)
                    else:
                        # å–®å€‹çµ„çš„æƒ…æ³
                        current_entry['name'] = name_match.group(1).strip()
                    break
        
        elif current_entry:
            # ç¹¼çºŒè™•ç†ç•¶å‰æ¢ç›®çš„å…¶ä»–è³‡è¨Š
            current_entry['raw_text'] += ' ' + line
            
            # å¦‚æœå§“åé‚„æ²’æ‰¾åˆ°ï¼Œç¹¼çºŒå˜—è©¦
            if not current_entry['name']:
                name_patterns = [
                    r'Name:\s*1:\s*([A-Z\s]+?)\s*2:\s*([A-Z\s]*?)\s*3:\s*([A-Z\s]*?)\s*4:\s*([A-Z\s]*?)(?:\s*Title:|DOB:|Designation:|Nationality:|$)',
                    r'Name:\s*([A-Z][A-Z\s]+?)(?:Title:|DOB:|Designation:|Nationality:|$)'
                ]
                for pattern in name_patterns:
                    name_match = re.search(pattern, line)
                    if name_match:
                        if len(name_match.groups()) > 1:
                            name_parts = [part.strip() for part in name_match.groups() if part and part.strip()]
                            current_entry['name'] = ' '.join(name_parts)
                        else:
                            current_entry['name'] = name_match.group(1).strip()
                        break
            
            # æå–åœ‹ç±
            nationality_match = re.search(r'Nationality:\s*([A-Za-z\s]+?)(?:\s+Passport|\s+National|\s+Address|\s+Listed|$)', line)
            if nationality_match:
                current_entry['nationality'] = nationality_match.group(1).strip()
            
            # æå–è­·ç…§è™Ÿç¢¼ (å¤šç¨®æ ¼å¼)
            passport_patterns = [
                r'Passport no:\s*([^\s]+(?:\s+number\s+[^\s]+)?)',
                r'Passport number\s*([^\s]+)',
                r'Passport:\s*([^\s]+)'
            ]
            for pattern in passport_patterns:
                passport_match = re.search(pattern, line, re.IGNORECASE)
                if passport_match:
                    passport_value = passport_match.group(1).strip()
                    if passport_value.lower() not in ['na', 'n/a', '']:
                        current_entry['passport_no'] = passport_value
                    break
    
    # è™•ç†æœ€å¾Œä¸€å€‹æ¢ç›®
    if current_entry:
        entries.append(finalize_entry(current_entry))
    
    return entries

def finalize_entry(entry):
    """å®Œæˆæ¢ç›®è™•ç†ï¼Œæ¸…ç†è³‡æ–™"""
    # æ¸…ç†åœ‹ç±
    if entry['nationality'] in ['Unknown', 'na', 'n/a', '']:
        entry['nationality'] = 'Unknown'
    
    # æ¸…ç†è­·ç…§è™Ÿç¢¼
    if entry['passport_no'] in ['na', 'n/a', '']:
        entry['passport_no'] = 'na'
    
    # ç¢ºä¿å§“åä¸ç‚ºç©º
    if not entry['name'].strip():
        entry['name'] = 'Unknown'
    
    return entry
